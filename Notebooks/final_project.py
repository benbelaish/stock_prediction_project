# -*- coding: utf-8 -*-
"""Stocks_Final_Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KwF-oqkc-mT2sKhJg79XWlNwXMvbxw5_
"""

# imports
import yfinance as yf
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, mean_absolute_error
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout
import numpy as np
import math
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import re
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

#download microsoft data
stock_data=yf.download('MSFT',start="2019-04-18",end="2020-04-18")

stock_data.info()
stock_data.head()

# Flattening MultiIndex Columns
new_columns=[stock_data.columns[i][0] for i in range(len(stock_data.columns))]
stock_data.columns=new_columns

stock_data.info()

#stock data correlations
print(f"Correlations: \n{stock_data.corr()}")

print(f"\nHeatmap: \n")
sns.heatmap(stock_data.corr(), annot=True, fmt='.3f', cmap='coolwarm')
plt.title('Correlation Matrix Heatmap')
plt.show()

#calculate the means by the months
open_mean = stock_data.groupby(pd.Grouper(freq='ME'))['Open'].mean()
close_mean = stock_data.groupby(pd.Grouper(freq='ME'))['Close'].mean()
volume_mean = stock_data.groupby(pd.Grouper(freq='ME'))['Volume'].mean()

#Price increase/decrease every day
diff = close_mean - open_mean

# Creating 3 graphs of close mean prices, volume mean values and differences of mean close and open prices in one row
figure, axis = plt.subplots(1, 3, figsize=(18, 4))

# Close mean prices
axis[0].plot(close_mean, label='close_mean')
axis[0].set_title('close_mean')

# Volume mean values
axis[1].plot(volume_mean, label='volume_mean')
axis[1].set_title('volume_mean')

# Bars representing price changes (red -> price dropped, green -> price rose)
colors = ['green' if x >= 0 else 'red' for x in diff.values]
axis[2].bar(diff.index, diff.values, color=colors)
axis[2].set_title('Difference (Close - Open)')
axis[2].axhline(y=0, color='black', linestyle='-', alpha=0.3) # Baseline

# Rotate the x ticks of each graphs
for ax in axis:
    ax.tick_params(axis='x', rotation=45)

# Display the graphs properly
plt.tight_layout()
axis[0].legend()
axis[1].legend()
plt.show()

# Add a target column
# Difference of close prices per day
stock_data['Change'] = stock_data['Close'] - stock_data['Close'].shift(1)

# Creating the target column representing classification of price actions
# Price rose -> 1
# Price dropped -> -1
# Unchanged price -> 0

stock_data['Target'] = 0  # Default - Unchanged price
stock_data.loc[stock_data['Change'] > 0, 'Target'] = 1  # Rise
stock_data.loc[stock_data['Change'] < 0, 'Target'] = -1  # Drop
stock_data.head()

# Drop first row because there isn't previous day to get change value
stock_data = stock_data.iloc[1:]

###################################
# Classical Classification Models #
###################################

# Select features, target and split the data for training and testing
features = ['Open', 'Close', 'Low', 'High', 'Volume']
target = 'Target'

X = stock_data[features]
y = stock_data[target]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Scaling the features values improving the model
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Logistic Regression
# C - Controls regularization
# Smaller C - increases regularization, reduces overfitting and makes the model simpler
# Larger C - weaker regularization, but allow to fit the model more closely, which can improve accuracy but may lead to overfitting
lr_model = LogisticRegression(C=100, random_state=42)
lr_model.fit(X_train, y_train)
lr_y_pred = lr_model.predict(X_test)
lr_accuracy = accuracy_score(y_test, lr_y_pred)
print(f"Test Accuracy Score: {lr_accuracy}\n")

# Check overfitting
lr_y_train_pred = lr_model.predict(X_train)
lr_train_accuracy = accuracy_score(y_train, lr_y_train_pred)
print(f"Train Accuracy Score: {lr_train_accuracy}")

# Random Forest Classifier
# n_estimators - the number of trees that will be created to 'vote' and classify
# max_depth - limit the depth of each tree to reduce overfitting
rfc_model = RandomForestClassifier(n_estimators=90, max_depth=5, random_state=42)
rfc_model.fit(X_train, y_train)
rfc_y_pred = rfc_model.predict(X_test)
rfc_accuracy = accuracy_score(y_test, rfc_y_pred)
print(f"Test Accuracy Score: {rfc_accuracy}\n")

# Check overfitting
rfc_y_train_pred = rfc_model.predict(X_train)
rfc_train_accuracy = accuracy_score(y_train, rfc_y_train_pred)
print(f"Train Accuracy Score: {rfc_train_accuracy}")

# Compare the models

# Logistic Regression Resutls
# Calculate the correct percentage of classification for each class
# average='weighted' - Returns the average of them
lr_f1 = f1_score(y_test, lr_y_pred, average='weighted')

# Random Forest Results
rfc_f1 = f1_score(y_test, rfc_y_pred, average='weighted')

# Print results
print("Model Performance Comparison \n")
print(f"Logistic Regression:")
print(f"- Accuracy: {lr_accuracy:.4f}")
print(f"- F1 Score: {lr_f1:.4f}")
print()
print(f"Random Forest:")
print(f"- Accuracy: {rfc_accuracy:.4f}")
print(f"- F1 Score: {rfc_f1:.4f}")

models = ['Logistic Regression', 'Random Forest']
accuracy_values = [lr_accuracy, rfc_accuracy]
train_accuracy_values = [lr_train_accuracy, rfc_train_accuracy]

# Set width of bars
bar_width = 0.3

# X locations for the groups
x = np.arange(len(models))

# Create bar chart
plt.figure(figsize=(7, 5))
plt.bar(x - bar_width/2, train_accuracy_values, width=bar_width, color='blue', alpha=0.7, label="Train Accuracy")
plt.bar(x + bar_width/2, accuracy_values, width=bar_width, color='green', alpha=0.7, label="Test Accuracy")

# Formatting
plt.xticks(x, models)  # Set x-axis labels
plt.ylim(0, 1)  # Accuracy is between 0 and 1
plt.title('Model Accuracy Comparison')
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.6)

# Show the plot
plt.show()

########
# LSTM #
########

# Setting window size
window_size = 5

# Select features, target and split the data for training and testing
features = ['Open', 'High', 'Low', 'Volume']
target = 'Close'

X_data = stock_data[features]
y_close = stock_data[target].values

# Scaling the features values improving the model
scaler_X = StandardScaler()
X_scaled = scaler_X.fit_transform(X_data)

scaler_y = StandardScaler()
y_scaled = scaler_y.fit_transform(y_close.reshape(-1, 1)).flatten()

# Creating the windows
X_sequences = []
y_sequences = []

for i in range(len(X_scaled) - window_size):
    X_sequences.append(X_scaled[i:i+window_size])
    y_sequences.append(y_scaled[i+window_size])

X_sequences = np.array(X_sequences)
y_sequences = np.array(y_sequences)

# Split data to training and testing
split = int(len(X_sequences) * 0.8)
X_train = X_sequences[:split]
X_test = X_sequences[split:]
y_train = y_sequences[:split]
y_test = y_sequences[split:]

# Creating and compile the LSTM model
model = Sequential([
    LSTM(100, input_shape=(window_size, len(features)), return_sequences=True),
    LSTM(50),
    Dropout(0.2),
    Dense(1)
])

model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error')

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Training the model
history = model.fit(
    X_train, y_train,
    epochs=100,
    batch_size=32,
    validation_data=(X_test, y_test),
    callbacks=[early_stop]
)

# Create predictions
y_pred_scaled = model.predict(X_test, verbose=0).flatten()
y_pred_train_scaled = model.predict(X_train, verbose=0).flatten()

# Convert predictions back to their original form
y_test_original = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()
y_pred_original = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
y_train_original = scaler_y.inverse_transform(y_train.reshape(-1, 1)).flatten()
y_pred_train_original = scaler_y.inverse_transform(y_pred_train_scaled.reshape(-1, 1)).flatten()

# Calculate model accuracy
mse = mean_squared_error(y_test_original, y_pred_original)
rmse = math.sqrt(mse)
mae = mean_absolute_error(y_test_original, y_pred_original)

mape = np.mean(np.abs((y_test_original - y_pred_original) / y_test_original)) * 100
train_mape = np.mean(np.abs((y_train_original - y_pred_train_original) / y_train_original)) * 100

# accuracy = 100% - error percentage
accuracy = 100 - mape
lstm_train_accuracy = 100 - train_mape

# Display the accuracy
print(f"LSTM model's results:")
print(f"RMSE: ${rmse:.2f}")
print(f"MAE: ${mae:.2f}")
print(f"LSTM model's accuracy: {accuracy:.2f}%")

# Check overfitting
print(f"LSTM accuracy - test: {accuracy:.2f}%")
print(f"LSTM accuracy - train: {lstm_train_accuracy:.2f}%")

# Graph - Comparison between real and predicted prices
plt.figure(figsize=(10, 5))
plt.plot(y_test_original, label='Real price', color='blue')
plt.plot(y_pred_original, label='Preticted price', color='red', alpha=0.7)
plt.title('Comparing actual prices versus predicted prices')
plt.xlabel('Time')
plt.ylabel('Price')
plt.legend()
plt.grid(True)
plt.show()

print()

# Graph - Comparison of validation and training loss
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Training Loss', color='blue')
plt.plot(history.history['val_loss'], label='Validation Loss', color='red')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Model loss curve')
plt.legend()
plt.grid()
plt.show()

#comparing between the models
mape = np.mean(np.abs((y_test_original - y_pred_original) / y_test_original)) * 100
lstm_accuracy = 100 - mape  # accuracy = 100% - error percentage

# Comparing the models' accuracy
print("\n===== Model Performance Comparison =====")
print(f"Logistic Regression accuracy: {lr_accuracy*100:.2f}%")
print(f"Random Forest accuracy: {rfc_accuracy*100:.2f}%")
print(f"LSTM model accuracy: {lstm_accuracy:.2f}%")

print("\nConclusions:")
print("• All models are shown in accuracy percentages for fair comparison")
print("• LSTM model is good at predicting exact values, while classical models are good at predicting trends")
print("• Using multiple models together can give a more complete picture")

# Display bar comparison of models' accuracy
def compare_models(models, models_accuracy, models_train_accuracy, title):
  # Set width of bars
  bar_width = 0.4

  # X locations for the groups
  x = np.arange(len(models))

  # Create bar chart
  plt.figure(figsize=(7, 5))
  train_accuracy_bars = plt.bar(x - bar_width/2, models_train_accuracy, width=bar_width, color='blue', label="Train Accuracy")
  test_accuracy_bars = plt.bar(x + bar_width/2, models_accuracy, width=bar_width, color='green', label="Test Accuracy")

  for i, bar in enumerate(train_accuracy_bars):
    alpha = models_train_accuracy[i] / 100
    bar.set_alpha(alpha)
    if alpha > 0.8:
      plt.text(i - bar_width/2, models_train_accuracy[i]-10, f"{models_train_accuracy[i]:.2f}%", ha='center', va='bottom', fontsize=12, color="white")
    else:
      plt.text(i - bar_width/2, models_train_accuracy[i]-10, f"{models_train_accuracy[i]:.2f}%", ha='center', va='bottom', fontsize=12)

  for i, bar in enumerate(test_accuracy_bars):
    alpha = models_accuracy[i] / 100
    bar.set_alpha(alpha)
    if alpha > 0.8:
      plt.text(i + bar_width/2, models_accuracy[i]-10, f"{models_accuracy[i]:.2f}%", ha='center', va='bottom', fontsize=12, color="white")
    else:
      plt.text(i + bar_width/2, models_accuracy[i]-10, f"{models_accuracy[i]:.2f}%", ha='center', va='bottom', fontsize=12)


  # Formatting
  plt.xticks(x, models)  # Set x-axis labels
  plt.ylim(0, 100)  # Accuracy is between 0 and 1
  plt.title(title)
  plt.xlabel('Models')
  plt.ylabel('Accuracy')
  plt.legend()

  # Show the plot
  plt.show()

# Compare models before sentiment
models = ["Logistic Regression", "Random Forest", "LSTM"]
models_accuracy = [lr_accuracy * 100, rfc_accuracy * 100, lstm_accuracy]
models_train_accuracy = [lr_train_accuracy * 100, rfc_train_accuracy * 100, lstm_train_accuracy]

compare_models(models, models_accuracy, models_train_accuracy, "Models accuracy before sentiment")

############
# News EDA #
############

# CNBC news EDA
cnbc_news_df = pd.read_csv('../Data/cnbc_headlines.csv')

cnbc_news_df.info()
cnbc_news_df.head()

# 280 lines that all the columns' values ara NaN
(cnbc_news_df.isna().sum(axis=1) == len(cnbc_news_df.columns)).sum()

# Delete NaN rows
cnbc_news_df = cnbc_news_df.dropna().reset_index(drop=True)

# Converting Time column to datetime data type
cnbc_news_df['Time'] = pd.to_datetime(cnbc_news_df['Time'])

cnbc_news_df.info()

# Guardian news EDA
guardian_news_df = pd.read_csv('../Data/guardian_headlines.csv')

guardian_news_df.info()
guardian_news_df.head()

# Converting Time column to datetime data type
guardian_news_df['Time'] = pd.to_datetime(guardian_news_df['Time'], errors='coerce')

guardian_news_df.info()

# Dropping NaN Time created due to coerce option
guardian_news_df = guardian_news_df.dropna().reset_index(drop=True)
guardian_news_df.info()

# Reuters news EDA
reuters_news_df = pd.read_csv('../Data/reuters_headlines.csv')

reuters_news_df.info()
reuters_news_df.head()

# Converting Time column to datetime data type
reuters_news_df['Time'] = pd.to_datetime(reuters_news_df['Time'])
reuters_news_df.info()

# Combining all Headlines
news_df = pd.concat([cnbc_news_df[['Time', 'Headlines']], guardian_news_df[['Time', 'Headlines']], reuters_news_df[['Time', 'Headlines']]], ignore_index=True)
news_df.info()
news_df.head()

# Clean headlines from stopwords (contain only meaningful words)

# Download stopwords list
nltk.download('punkt_tab')
nltk.download('wordnet')
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Initialize lemmatizer
lemmatizer = WordNetLemmatizer()

# Cleaning the headlines
def clean_headlines(headline):
  headline = headline.lower()
  headline = re.sub(r"[^a-z0-9\s]", "", headline)
  words = word_tokenize(headline)
  words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]
  return " ".join(words)

news_df['clean_headlines'] = news_df['Headlines'].apply(clean_headlines)

news_df.head()

# Create Tokenizer and fit it to the headlines
tokenizer = Tokenizer(oov_token="<OOV>")
tokenizer.fit_on_texts(news_df['clean_headlines'])

# Display the 10 most popular words
word_counts = tokenizer.word_counts
top_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:10]
print(f"\nTop 10 most common words:\n")
print("word: # of occurrences\n")
for i, (word, occurrences) in enumerate(top_words[:10]):
  print(f"{i+1}) {word}: {occurrences}")

# Convert the headlines to sequences
sequences = tokenizer.texts_to_sequences(news_df['clean_headlines'])

# Getting max length sequence
maxlen = max(len(seq) for seq in sequences)

# Padding the sequences to be same length for the model
padded_sequences = pad_sequences(sequences, padding='post', maxlen=maxlen)

# LSTM model for headline sentiment

X = np.array(padded_sequences)

# Create the model
sentiment_model = Sequential([
    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=32),
    LSTM(64),
    Dense(32, activation ='relu'),
    Dense(1, activation='sigmoid')
])

# Unsupervised predictions
sentiment_preds = sentiment_model.predict(padded_sequences)

plt.hist(sentiment_preds, bins="auto")
plt.title("Sentiment predictions")
plt.xlabel("sentiment prediction")
plt.ylabel("occurrences")
plt.show()

news_df['sentiment'] = sentiment_preds

news_df.info()
news_df.head()

# Getting only the relevant daily average sentiments based on trading days
news_df['Time'] = pd.to_datetime(news_df['Time'])
time_mask = pd.to_datetime(stock_data.index)
relevant_news = news_df[news_df['Time'].isin(time_mask)]
daily_sentiment = relevant_news.groupby('Time')['sentiment'].mean()

# Adding the sentiment values to each trading day(row)
stock_data['sentiment'] = daily_sentiment
stock_data.info()
stock_data.head()

plt.figure(figsize=(10,6))
plt.plot(stock_data['sentiment'])
plt.title('Sentiment over time')
plt.xlabel("date")
plt.ylabel("average sentiment")
plt.show()

# Calculate and display a plot of monthly sentiment mean
sentiment_mean = stock_data.groupby(pd.Grouper(freq='ME'))['sentiment'].mean()

plt.figure(figsize=(10, 6))
plt.plot(sentiment_mean)
plt.title('Average sentiment each month over time')
plt.xlabel("date")
plt.ylabel("average sentiment")
plt.show()

# Classical models with sentiment

# Select features, target and split the data for training and testing
features = ['Open', 'Close', 'Low', 'High', 'Volume', 'sentiment']
target = 'Target'

X = stock_data[features]
y = stock_data[target]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Scaling the features values improving the model
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Logistic Regression
# C - Controls regularization
# Smaller C - increases regularization, reduces overfitting and makes the model simpler
# Larger C - weaker regularization, but allow to fit the model more closely, which can improve accuracy but may lead to overfitting
lr_model = LogisticRegression(C=100, random_state=42)
lr_model.fit(X_train, y_train)
lr_y_pred = lr_model.predict(X_test)
lr_sent_accuracy = accuracy_score(y_test, lr_y_pred)
print(f"Test Accuracy Score: {lr_sent_accuracy}\n")

# Check overfitting
lr_y_train_pred = lr_model.predict(X_train)
lr_train_sent_accuracy = accuracy_score(y_train, lr_y_train_pred)
print(f"Train Accuracy Score: {lr_train_accuracy}")

# Random Forest Classifier
# n_estimators - the number of trees that will be created to 'vote' and classify
# max_depth - limit the depth of each tree to reduce overfitting
rfc_model = RandomForestClassifier(n_estimators=90, max_depth=5, random_state=42)
rfc_model.fit(X_train, y_train)
rfc_y_pred = rfc_model.predict(X_test)
rfc_sent_accuracy = accuracy_score(y_test, rfc_y_pred)
print(f"Test Accuracy Score: {rfc_sent_accuracy}\n")

# Check overfitting
rfc_y_train_pred = rfc_model.predict(X_train)
rfc_train_sent_accuracy = accuracy_score(y_train, rfc_y_train_pred)
print(f"Train Accuracy Score: {rfc_train_accuracy}")

# LSTM with sentiment

# Setting window size
window_size = 5

# Select features, target and split the data for training and testing
features = ['Open', 'High', 'Low', 'Volume', 'sentiment']
target = 'Close'

X_data = stock_data[features]
y_close = stock_data[target].values

# Scaling the features values improving the model
scaler_X = StandardScaler()
X_scaled = scaler_X.fit_transform(X_data)

scaler_y = StandardScaler()
y_scaled = scaler_y.fit_transform(y_close.reshape(-1, 1)).flatten()

# Creating the windows
X_sequences = []
y_sequences = []

for i in range(len(X_scaled) - window_size):
    X_sequences.append(X_scaled[i:i+window_size])
    y_sequences.append(y_scaled[i+window_size])

X_sequences = np.array(X_sequences)
y_sequences = np.array(y_sequences)

# Split data to training and testing
split = int(len(X_sequences) * 0.8)
X_train = X_sequences[:split]
X_test = X_sequences[split:]
y_train = y_sequences[:split]
y_test = y_sequences[split:]

# Creating and compile the LSTM model
model = Sequential([
    LSTM(100, input_shape=(window_size, len(features)), return_sequences=True),
    LSTM(50),
    Dropout(0.2),
    Dense(1)
])

model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error')

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Training the model
history = model.fit(
    X_train, y_train,
    epochs=100,
    batch_size=32,
    validation_data=(X_test, y_test),
    callbacks=[early_stop]
)

# Create predictions
y_pred_scaled = model.predict(X_test, verbose=0).flatten()
y_pred_train_scaled = model.predict(X_train, verbose=0).flatten()

# Convert predictions back to their original form
y_test_original = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()
y_pred_original = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
y_train_original = scaler_y.inverse_transform(y_train.reshape(-1, 1)).flatten()
y_pred_train_original = scaler_y.inverse_transform(y_pred_train_scaled.reshape(-1, 1)).flatten()

# Calculate model accuracy
mse = mean_squared_error(y_test_original, y_pred_original)
rmse = math.sqrt(mse)
mae = mean_absolute_error(y_test_original, y_pred_original)

mape = np.mean(np.abs((y_test_original - y_pred_original) / y_test_original)) * 100
train_mape = np.mean(np.abs((y_train_original - y_pred_train_original) / y_train_original)) * 100


# accuracy = 100% - error percentage
lstm_sent_accuracy = 100 - mape
lstm_sent_train_accuracy = 100 - train_mape

# Display the accuracy
print(f"LSTM model's results:")
print(f"RMSE: ${rmse:.2f}")
print(f"MAE: ${mae:.2f}")
print(f"LSTM model's accuracy: {lstm_sent_accuracy:.2f}%")

# Check overfitting
print("\nOverfitting check:")
print(f"LSTM accuracy - test: {lstm_sent_accuracy:.2f}%")
print(f"LSTM accuracy - train: {lstm_sent_train_accuracy:.2f}%")

# Graph - Comparison between real and predicted prices
plt.figure(figsize=(10, 5))
plt.plot(y_test_original, label='Real price', color='blue')
plt.plot(y_pred_original, label='Preticted price', color='red', alpha=0.7)
plt.title('Comparing actual prices versus predicted prices after sentiment')
plt.xlabel('Time')
plt.ylabel('Price')
plt.legend()
plt.grid(True)
plt.show()

print()

# Graph - Comparison of validation and training loss
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Training Loss', color='blue')
plt.plot(history.history['val_loss'], label='Validation Loss', color='red')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Model loss curve after sentiment')
plt.legend()
plt.grid()
plt.show()

#comparing between the models

# Comparing the models' accuracy
print("===== Model Performance Comparison =====")
print(f"Logistic Regression accuracy: {lr_sent_accuracy*100:.2f}%")
print(f"Random Forest accuracy: {rfc_sent_accuracy*100:.2f}%")
print(f"LSTM model accuracy: {lstm_sent_accuracy:.2f}%")

print("\nConclusions:")
print("• The sentiment seems to decrease the accuracy by a little")
print("• The sentiment wasn't that helpful probably because of the unsupervised learning technique")
print("• LSTM model is good at predicting exact values, while classical models are good at predicting trends")

# Compare models before sentiment
models = ["Logistic Regression", "Random Forest", "LSTM"]
models_accuracy = [lr_accuracy * 100, rfc_accuracy * 100, lstm_accuracy]
models_train_accuracy = [lr_train_accuracy * 100, rfc_train_accuracy * 100, lstm_train_accuracy]

compare_models(models, models_accuracy, models_train_accuracy, "Models accuracy before sentiment")

# Compare models after sentiment
models = ["Logistic Regression", "Random Forest", "LSTM"]
models_accuracy = [lr_sent_accuracy * 100, rfc_sent_accuracy * 100, lstm_sent_accuracy]
models_train_accuracy = [lr_train_sent_accuracy * 100, rfc_train_sent_accuracy * 100, lstm_sent_train_accuracy]

compare_models(models, models_accuracy, models_train_accuracy, "Models accuracy after sentiment")

